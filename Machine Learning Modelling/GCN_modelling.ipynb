{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK1nCcR3xWLI",
        "outputId": "bbccc65c-2551-48dc-f1e0-65d2fed952e6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
        "import torch\n",
        "from torch_geometric.data import Data, InMemoryDataset\n",
        "from torch_geometric.utils import from_networkx\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "5TpZon93xWLK",
        "outputId": "6915553e-6b3b-432a-ba75-ae67fc74733c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>from</th>\n",
              "      <th>to</th>\n",
              "      <th>contract_address</th>\n",
              "      <th>cumulativeGasUsed</th>\n",
              "      <th>gasUsed</th>\n",
              "      <th>Date</th>\n",
              "      <th>Value_usd</th>\n",
              "      <th>Transaction_cost_usd</th>\n",
              "      <th>isError</th>\n",
              "      <th>isinternal</th>\n",
              "      <th>...</th>\n",
              "      <th>Date_born</th>\n",
              "      <th>age_days</th>\n",
              "      <th>age_minutes</th>\n",
              "      <th>user</th>\n",
              "      <th>cluster_0.0</th>\n",
              "      <th>cluster_1.0</th>\n",
              "      <th>cluster_2.0</th>\n",
              "      <th>cluster_3.0</th>\n",
              "      <th>cluster_4.0</th>\n",
              "      <th>cluster_5.0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0xd2e6b3bfe990fdede2380885d9d83ca9364e717e</td>\n",
              "      <td>0x00efd61b0d94ccd82f3922d26efdd3ed9859081a</td>\n",
              "      <td>0x00efd61b0d94ccd82f3922d26efdd3ed9859081a</td>\n",
              "      <td>7123739.0</td>\n",
              "      <td>1515366</td>\n",
              "      <td>2018-04-10 17:55:17</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.945962</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2018-04-10 17:55:17</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0xd2e6b3bfe990fdede2380885d9d83ca9364e717e</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0x20c945800de43394f70d789874a4dac9cfa57451</td>\n",
              "      <td>0x00efd61b0d94ccd82f3922d26efdd3ed9859081a</td>\n",
              "      <td>0x00efd61b0d94ccd82f3922d26efdd3ed9859081a</td>\n",
              "      <td>3069896.0</td>\n",
              "      <td>21000</td>\n",
              "      <td>2018-04-10 18:14:01</td>\n",
              "      <td>2.858277e-14</td>\n",
              "      <td>0.008699</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2018-04-10 17:55:17</td>\n",
              "      <td>0</td>\n",
              "      <td>18.733333</td>\n",
              "      <td>0x20c945800de43394f70d789874a4dac9cfa57451</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0x0668dea6b5ec94d7ce3c43fe477888eee2fc1b2c</td>\n",
              "      <td>0x00efd61b0d94ccd82f3922d26efdd3ed9859081a</td>\n",
              "      <td>0x00efd61b0d94ccd82f3922d26efdd3ed9859081a</td>\n",
              "      <td>243354.0</td>\n",
              "      <td>21110</td>\n",
              "      <td>2018-04-10 18:39:40</td>\n",
              "      <td>1.739821e-13</td>\n",
              "      <td>0.008745</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2018-04-10 17:55:17</td>\n",
              "      <td>0</td>\n",
              "      <td>44.383333</td>\n",
              "      <td>0x0668dea6b5ec94d7ce3c43fe477888eee2fc1b2c</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0xd2e6b3bfe990fdede2380885d9d83ca9364e717e</td>\n",
              "      <td>0x00efd61b0d94ccd82f3922d26efdd3ed9859081a</td>\n",
              "      <td>0x00efd61b0d94ccd82f3922d26efdd3ed9859081a</td>\n",
              "      <td>179688.0</td>\n",
              "      <td>127688</td>\n",
              "      <td>2018-04-10 21:17:03</td>\n",
              "      <td>1.242729e+02</td>\n",
              "      <td>0.428440</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2018-04-10 17:55:17</td>\n",
              "      <td>0</td>\n",
              "      <td>201.766667</td>\n",
              "      <td>0xd2e6b3bfe990fdede2380885d9d83ca9364e717e</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0xc951d3463ebba4e9ec8ddfe1f42bc5895c46ec8f</td>\n",
              "      <td>0x00efd61b0d94ccd82f3922d26efdd3ed9859081a</td>\n",
              "      <td>0x00efd61b0d94ccd82f3922d26efdd3ed9859081a</td>\n",
              "      <td>3752942.0</td>\n",
              "      <td>27046</td>\n",
              "      <td>2018-04-10 21:18:04</td>\n",
              "      <td>1.367002e+03</td>\n",
              "      <td>0.090525</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2018-04-10 17:55:17</td>\n",
              "      <td>0</td>\n",
              "      <td>202.783333</td>\n",
              "      <td>0xc951d3463ebba4e9ec8ddfe1f42bc5895c46ec8f</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         from  \\\n",
              "0  0xd2e6b3bfe990fdede2380885d9d83ca9364e717e   \n",
              "1  0x20c945800de43394f70d789874a4dac9cfa57451   \n",
              "2  0x0668dea6b5ec94d7ce3c43fe477888eee2fc1b2c   \n",
              "3  0xd2e6b3bfe990fdede2380885d9d83ca9364e717e   \n",
              "4  0xc951d3463ebba4e9ec8ddfe1f42bc5895c46ec8f   \n",
              "\n",
              "                                           to  \\\n",
              "0  0x00efd61b0d94ccd82f3922d26efdd3ed9859081a   \n",
              "1  0x00efd61b0d94ccd82f3922d26efdd3ed9859081a   \n",
              "2  0x00efd61b0d94ccd82f3922d26efdd3ed9859081a   \n",
              "3  0x00efd61b0d94ccd82f3922d26efdd3ed9859081a   \n",
              "4  0x00efd61b0d94ccd82f3922d26efdd3ed9859081a   \n",
              "\n",
              "                             contract_address  cumulativeGasUsed  gasUsed  \\\n",
              "0  0x00efd61b0d94ccd82f3922d26efdd3ed9859081a          7123739.0  1515366   \n",
              "1  0x00efd61b0d94ccd82f3922d26efdd3ed9859081a          3069896.0    21000   \n",
              "2  0x00efd61b0d94ccd82f3922d26efdd3ed9859081a           243354.0    21110   \n",
              "3  0x00efd61b0d94ccd82f3922d26efdd3ed9859081a           179688.0   127688   \n",
              "4  0x00efd61b0d94ccd82f3922d26efdd3ed9859081a          3752942.0    27046   \n",
              "\n",
              "                  Date     Value_usd  Transaction_cost_usd  isError  \\\n",
              "0  2018-04-10 17:55:17  0.000000e+00              1.945962        0   \n",
              "1  2018-04-10 18:14:01  2.858277e-14              0.008699        1   \n",
              "2  2018-04-10 18:39:40  1.739821e-13              0.008745        1   \n",
              "3  2018-04-10 21:17:03  1.242729e+02              0.428440        0   \n",
              "4  2018-04-10 21:18:04  1.367002e+03              0.090525        1   \n",
              "\n",
              "   isinternal  ...            Date_born  age_days age_minutes  \\\n",
              "0           0  ...  2018-04-10 17:55:17         0    0.000000   \n",
              "1           0  ...  2018-04-10 17:55:17         0   18.733333   \n",
              "2           0  ...  2018-04-10 17:55:17         0   44.383333   \n",
              "3           0  ...  2018-04-10 17:55:17         0  201.766667   \n",
              "4           0  ...  2018-04-10 17:55:17         0  202.783333   \n",
              "\n",
              "                                         user  cluster_0.0 cluster_1.0  \\\n",
              "0  0xd2e6b3bfe990fdede2380885d9d83ca9364e717e            0           0   \n",
              "1  0x20c945800de43394f70d789874a4dac9cfa57451            0           0   \n",
              "2  0x0668dea6b5ec94d7ce3c43fe477888eee2fc1b2c            0           0   \n",
              "3  0xd2e6b3bfe990fdede2380885d9d83ca9364e717e            0           0   \n",
              "4  0xc951d3463ebba4e9ec8ddfe1f42bc5895c46ec8f            0           0   \n",
              "\n",
              "   cluster_2.0  cluster_3.0  cluster_4.0  cluster_5.0  \n",
              "0            0            0            1            0  \n",
              "1            0            0            1            0  \n",
              "2            0            0            1            0  \n",
              "3            0            0            1            0  \n",
              "4            0            0            1            0  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "complete_dataset = pd.read_csv('all_cleaned_data.csv')\n",
        "complete_dataset.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Required Data Preparation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sMUwFRxPOhiM"
      },
      "outputs": [],
      "source": [
        "def transaction_values_sent(df):\n",
        "  return df.groupby('from')[['Value_usd']].agg(['count', 'sum', 'mean', 'std', 'median', 'max', 'min']).reset_index().fillna(0).rename({'from':'identifier'}, axis=1)\n",
        "\n",
        "def transaction_values_received(df):\n",
        "  return df.groupby('to')[['Value_usd']].agg(['count', 'sum', 'mean', 'std', 'median', 'max', 'min']).reset_index().fillna(0).rename({'to':'identifier'}, axis=1)\n",
        "\n",
        "def errors_sent(df):\n",
        "  return df.groupby('from')[['isError']].agg(['sum', 'mean', 'std', 'median']).reset_index().fillna(0).rename({'from':'identifier'}, axis=1)\n",
        "\n",
        "def errors_received(df):\n",
        "  return df.groupby('to')[['isError']].agg(['sum', 'mean', 'std', 'median']).reset_index().fillna(0).rename({'to':'identifier'}, axis=1)\n",
        "\n",
        "def lifetimes_sent(df):\n",
        "  return df.groupby('from')[['age_minutes']].agg(['mean', 'std', 'median']).reset_index().fillna(0).rename({'from':'identifier'}, axis=1)\n",
        "\n",
        "def lifetimes_received(df):\n",
        "  return df.groupby('to')[['age_minutes']].agg(['mean', 'std', 'median']).reset_index().fillna(0).rename({'to':'identifier'}, axis=1)\n",
        "\n",
        "def transaction_costs_sent(df):\n",
        "  return df.groupby('from')[['Transaction_cost_usd']].agg(['sum', 'mean', 'std', 'median', 'max', 'min']).reset_index().fillna(0).rename({'from':'identifier'}, axis=1)\n",
        "def degree_calculator(df):\n",
        "    return df.shape[0]\n",
        "\n",
        "def number_transacts_user(df):\n",
        "    if df.shape[0]==0:\n",
        "        return 0,0,0,0,0,0,0\n",
        "    td = df.groupby('user')[['to']].count().reset_index()\n",
        "    return td['to'].mean(), td['to'].std(), td['to'].median(),  td['to'].min(), np.quantile(td['to'], q=0.25), np.quantile(td['to'], q=0.75), td['to'].max()\n",
        "\n",
        "def errors_calculator(df):\n",
        "    if df.shape[0]==0:\n",
        "        return 0,0,0,0,0,0\n",
        "    return df['isError'].sum(), df['isError'].mean(), df['isError'].std(), df['isError'].median(), np.quantile(df['isError'], q=0.25), np.quantile(df['isError'], q=0.75)\n",
        "\n",
        "def distinct_users(df):\n",
        "    if df.shape[0]==0:\n",
        "        return 0\n",
        "    return df['user'].nunique()\n",
        "\n",
        "def transaction_value(df):\n",
        "    if df.shape[0]==0:\n",
        "        return 0,0,0,0,0,0,0,0\n",
        "    return df['Value_usd'].sum(), df['Value_usd'].mean(), df['Value_usd'].std(), df['Value_usd'].median(), df['Value_usd'].min(), np.quantile(df['Value_usd'], q=0.25), np.quantile(df['Value_usd'], q=0.75), df['Value_usd'].max()\n",
        "\n",
        "def lifetime_calculator(df):\n",
        "    return df['age_minutes'].mean(), df['age_minutes'].std(), df['age_minutes'].median(), np.quantile(df['age_minutes'], q=0.25), np.quantile(df['age_minutes'], q=0.75), df['age_minutes'].max()\n",
        "\n",
        "def sign_calculator(row):\n",
        "    if row['from']==row['user']:\n",
        "        return -row['Value_usd']\n",
        "    else:\n",
        "        return row['Value_usd']\n",
        "\n",
        "def inflows_from_users(df):\n",
        "    td = df[df['value_correct_sign']<0]\n",
        "    if td.shape[0]==0:\n",
        "        return 0,0,0,0,0,0,0\n",
        "    td = td.groupby('user')[['value_correct_sign']].sum().reset_index()\n",
        "    return td['value_correct_sign'].mean(), td['value_correct_sign'].std(), td['value_correct_sign'].median(),  td['value_correct_sign'].min(), np.quantile(td['value_correct_sign'], q=0.25), np.quantile(td['value_correct_sign'], q=0.75), td['value_correct_sign'].max()\n",
        "\n",
        "def outflows_to_users(df):\n",
        "    td = df[df['value_correct_sign']>0]\n",
        "    if td.shape[0]==0:\n",
        "        return 0,0,0,0,0,0,0\n",
        "    td = td.groupby('user')[['value_correct_sign']].sum().reset_index()\n",
        "    return td['value_correct_sign'].mean(), td['value_correct_sign'].std(), td['value_correct_sign'].median(),  td['value_correct_sign'].min(), np.quantile(td['value_correct_sign'], q=0.25), np.quantile(td['value_correct_sign'], q=0.75), td['value_correct_sign'].max()\n",
        "\n",
        "def transaction_costs_contract(df):\n",
        "    if df.shape[0]==0:\n",
        "        return 0,0,0,0,0,0,0,0\n",
        "    return df['Transaction_cost_usd'].sum(), df['Transaction_cost_usd'].mean(), df['Transaction_cost_usd'].std(), df['Transaction_cost_usd'].median(), df['Transaction_cost_usd'].min(),np.quantile(df['Transaction_cost_usd'], q=0.25), np.quantile(df['Transaction_cost_usd'], q=0.75), df['Transaction_cost_usd'].max()\n",
        "\n",
        "def transaction_costs_users(df):\n",
        "    if df.shape[0]==0:\n",
        "        return 0,0,0,0,0,0,0\n",
        "    td = df.groupby('user')[['Transaction_cost_usd']].sum().reset_index()\n",
        "    return td['Transaction_cost_usd'].mean(), td['Transaction_cost_usd'].std(), td['Transaction_cost_usd'].median(),  td['Transaction_cost_usd'].min(), np.quantile(td['Transaction_cost_usd'], q=0.25), np.quantile(td['Transaction_cost_usd'], q=0.75), td['Transaction_cost_usd'].max()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y3HsPNdPR4Qu"
      },
      "outputs": [],
      "source": [
        "def aggregate_edges(temp_df):\n",
        "  td1 = temp_df.groupby('from')[['Value_usd']].sum().reset_index().rename({'from':'idx', 'Value_usd':'sent'}, axis=1)\n",
        "  td2 = temp_df.groupby('to')[['Value_usd']].sum().reset_index().rename({'to':'idx', 'Value_usd':'received'}, axis=1)\n",
        "  td2=pd.merge(td1, td2, on='idx', how='outer').fillna(0)\n",
        "  td2=td2[td2['idx']!=temp_df['contract_address'].iloc[0]]\n",
        "  td2['value']=td2['received']-td2['sent']\n",
        "  froms = td2[td2['value']<0] #those are nodes that have only paid. There should be an edge from the node to the contract\n",
        "  froms['to']=temp_df['contract_address'].iloc[0]\n",
        "  froms['value']*=-1\n",
        "  froms = froms.rename({'idx':'from'}, axis=1)\n",
        "  tos = td2[td2['value']>=0]\n",
        "  tos['from']=temp_df['contract_address'].iloc[0]\n",
        "  tos['value']+=1e-10\n",
        "  tos = tos.rename({'idx':'to'}, axis=1)\n",
        "  td = pd.concat((froms, tos)).reset_index(drop=True)\n",
        "  return td"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dBtqnLnnlSgH"
      },
      "outputs": [],
      "source": [
        "complete_dataset=complete_dataset.fillna(0)\n",
        "\n",
        "def create_data(step):\n",
        "  graphs_data = []\n",
        "  labels_data = []\n",
        "  graph_idx = []\n",
        "  for contract in list(set(complete_dataset['contract_address'])):\n",
        "      temp_df = complete_dataset[complete_dataset['contract_address']==contract].reset_index(drop=True).head(10*step)\n",
        "      if temp_df.shape[0]>=10*step:\n",
        "        G=nx.DiGraph()\n",
        "        #nx.set_node_attributes(G, {node:{x:[attributes]}})\n",
        "        #adding node features\n",
        "        node_attributes = pd.merge(transaction_values_sent(temp_df), transaction_values_received(temp_df), on='identifier', how='outer')\n",
        "        node_attributes['account_balance']=node_attributes.loc[:, ('Value_usd_x', 'sum')] - node_attributes.loc[:, ('Value_usd_y', 'sum')]\n",
        "        node_attributes = pd.merge(node_attributes, errors_sent(temp_df), on='identifier', how='outer')\n",
        "        node_attributes = pd.merge(node_attributes, errors_received(temp_df), on='identifier', how='outer')\n",
        "        node_attributes = pd.merge(node_attributes, lifetimes_sent(temp_df), on='identifier', how='outer')\n",
        "        node_attributes = pd.merge(node_attributes, lifetimes_received(temp_df), on='identifier', how='outer')\n",
        "        node_attributes = pd.merge(node_attributes, transaction_costs_sent(temp_df), on='identifier', how='outer')\n",
        "        #node_attributes['distinct_receiving_users'] = temp_df.groupby('from')[['to']].nunique()['to'].tolist()\n",
        "        cluster_data = temp_df.iloc[0, -6:].tolist()\n",
        "        node_attributes['cluster_0']=cluster_data[0]\n",
        "        node_attributes['cluster_1']=cluster_data[1]\n",
        "        node_attributes['cluster_2']=cluster_data[2]\n",
        "        node_attributes['cluster_3']=cluster_data[3]\n",
        "        node_attributes['cluster_4']=cluster_data[4]\n",
        "        node_attributes['cluster_5']=cluster_data[5]\n",
        "        node_attributes=node_attributes.fillna(0)\n",
        "\n",
        "        aggregated_edges = aggregate_edges(temp_df)\n",
        "\n",
        "        for j in range(aggregated_edges.shape[0]):\n",
        "          #adding edges with their feaures\n",
        "          from_node = aggregated_edges.loc[j, 'from']\n",
        "          to_node = aggregated_edges.loc[j, 'to']\n",
        "          edge_characts = aggregated_edges.loc[j, 'value']\n",
        "          G.add_edge(from_node, to_node, edge_attr=edge_characts)\n",
        "\n",
        "\n",
        "        for idx in node_attributes['identifier']:\n",
        "          attrs = {idx: {'x':node_attributes[node_attributes['identifier']==idx].iloc[0, :].tolist()[1:]}}\n",
        "          nx.set_node_attributes(G, attrs)\n",
        "\n",
        "        temp_df['value_correct_sign'] = temp_df.apply(sign_calculator, axis=1)\n",
        "        temp_df_internal = temp_df[temp_df['isinternal']==1]\n",
        "        temp_df_external = temp_df[temp_df['isinternal']==0]\n",
        "        graph_attributes = []\n",
        "        #degrees\n",
        "        contract_in_degrees=degree_calculator(temp_df_external)\n",
        "        contract_out_degrees=degree_calculator(temp_df_internal)\n",
        "        #number_transacts_users_indeg\n",
        "        avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = number_transacts_user(temp_df_external)\n",
        "        graph_attributes+=[avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #number_transacts_users_outdeg\n",
        "        avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = number_transacts_user(temp_df_internal)\n",
        "        graph_attributes+=[avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #errors_indeg\n",
        "        total_numb,avg_numb, std_numb, median_numb, q1_numb, q3_numb = errors_calculator(temp_df_external)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, q1_numb, q3_numb]\n",
        "        #errors_outdeg\n",
        "        total_numb,avg_numb, std_numb, median_numb, q1_numb, q3_numb = errors_calculator(temp_df_internal)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, q1_numb, q3_numb]\n",
        "        #distinct_users\n",
        "        distinct_users_indeg=distinct_users(temp_df_external)\n",
        "        distinct_users_outdeg=distinct_users(temp_df_internal)\n",
        "        graph_attributes+=[distinct_users_indeg, distinct_users_outdeg]\n",
        "        #transaction_value_indeg\n",
        "        total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = transaction_value(temp_df_external)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #transaction_value_outdeg\n",
        "        total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = transaction_value(temp_df_internal)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #age_minutes\n",
        "        avg_numb, std_numb, median_numb, q1_numb, q3_numb, max_numb = lifetime_calculator(temp_df)\n",
        "        graph_attributes+=[avg_numb, std_numb, median_numb, q1_numb, q3_numb, max_numb]\n",
        "        #inflows_from_users\n",
        "        avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = inflows_from_users(temp_df_external)\n",
        "        graph_attributes+=[avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #outflows_from_users\n",
        "        avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = outflows_to_users(temp_df_internal)\n",
        "        graph_attributes+=[avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #transaction_costs_contract\n",
        "        total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = transaction_costs_contract(temp_df_internal)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #transaction_costs_users\n",
        "        avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = transaction_costs_users(temp_df_external)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        G.graph.update({'contract_features':graph_attributes})\n",
        "\n",
        "        label = temp_df.is_ponzi.iloc[0]\n",
        "        graphs_data.append(G)\n",
        "        labels_data.append(label)\n",
        "        graph_idx.append(contract)\n",
        "  return graphs_data, labels_data, graph_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j75a7_pXxWLU"
      },
      "outputs": [],
      "source": [
        "class FraudDataset(InMemoryDataset):\n",
        "    def __init__(self, graphs, labels, idxs):\n",
        "        self.graphs = graphs\n",
        "        self.labels = labels\n",
        "        self.idxs = idxs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        graph = self.graphs[index]\n",
        "        label = self.labels[index]\n",
        "        data = from_networkx(graph)\n",
        "        data['label']=label\n",
        "        data['idx']=self.idxs[index]\n",
        "        return data\n",
        "\n",
        "    def get_labels(self):\n",
        "      return self.labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w1A4oqt0XNTi"
      },
      "outputs": [],
      "source": [
        "def data_splitter(graphs_data, labels_data, graph_idx):\n",
        "  train, test = train_test_split(pd.DataFrame({'label':labels_data}).reset_index(), test_size=0.25, random_state=42)\n",
        "  train_data_graph = []\n",
        "  train_data_label = []\n",
        "  train_data_idx = []\n",
        "  for i in train['index'].tolist():\n",
        "    train_data_graph.append(graphs_data[i])\n",
        "    train_data_label.append(labels_data[i])\n",
        "    train_data_idx.append(graph_idx[i])\n",
        "\n",
        "  test_data_graph = []\n",
        "  test_data_label = []\n",
        "  test_data_idx = []\n",
        "  for i in test['index'].tolist():\n",
        "    test_data_graph.append(graphs_data[i])\n",
        "    test_data_label.append(labels_data[i])\n",
        "    test_data_idx.append(graph_idx[i])\n",
        "\n",
        "  dataset_train = FraudDataset(train_data_graph, train_data_label, train_data_idx)\n",
        "  dataset_test = FraudDataset(test_data_graph, test_data_label, test_data_idx)\n",
        "  train_dataloader = DataLoader(dataset_train, batch_size=500, shuffle=True)\n",
        "  test_dataloader = DataLoader(dataset_test, batch_size=100, shuffle=False)\n",
        "  return train_dataloader, test_dataloader"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KrHfur82fPxl"
      },
      "source": [
        "### GCN\n",
        "* mean aggregation captures the distribution (or proportions) of elements\n",
        "* max aggregation proves to be advantageous to identify representative elements\n",
        "* sum aggregation enables the learning of structural graph properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wt1jjo3SjPSH"
      },
      "outputs": [],
      "source": [
        "class GCN_mean(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, dropout_frac=0.5):\n",
        "        super(GCN_mean, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "        self.conv1 = GCNConv(41, hidden_channels) #41=node features\n",
        "        self.lin1 = Linear(hidden_channels+80, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, 2) #2=number of classes\n",
        "        self.dropout_frac=dropout_frac\n",
        "\n",
        "    def forward(self, data):\n",
        "        x=data.x.to(torch.float32)\n",
        "        x = self.conv1(x, data.edge_index)\n",
        "        x = x.relu()\n",
        "        x = global_mean_pool(x, data.batch)\n",
        "        contract_feats=data.contract_features.to(torch.float32)\n",
        "        contract_feats = contract_feats.reshape(len(data.label), 80)\n",
        "        contract_feats[torch.isnan(contract_feats)] = 0\n",
        "        x = torch.cat((x, contract_feats), dim=1)\n",
        "        x =self.lin1(x)\n",
        "        x = F.dropout(x, p=self.dropout_frac, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "def train(model, train_dataloader, device, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    for data in train_dataloader:  # Iterate in batches over the training dataset.\n",
        "         data = data.to(device)\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "         out = model(data)  # Perform a single forward pass.\n",
        "         y_label = torch.tensor(data.label).to(device)\n",
        "         #loss = F.nll_loss(out, y_label)\n",
        "         loss = F.nll_loss(out, y_label)\n",
        "         #loss = criterion(out, y_label)  # Compute the loss.\n",
        "         #loss=criterion(torch.argmax(out, dim=1), y_label)\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "\n",
        "def test(model, loader, device):\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  outs = []\n",
        "  true_y=[]\n",
        "  for data in loader:\n",
        "      data = data.to(device)\n",
        "      out = model(data)\n",
        "      pred = out.argmax(dim=1)\n",
        "      preds+=pred.tolist()\n",
        "      true_y+=data.label\n",
        "      outs+=out.tolist()\n",
        "  outs=np.array(outs)\n",
        "  return recall_score(true_y, preds), precision_score(true_y, preds), roc_auc_score(true_y,outs[:,1]), f1_score(true_y, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def do_step_mean(train_dataloader, test_dataloader, step, metrics_df = pd.DataFrame()):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  drop_rates = [0.1,0.2,0.3,0.4]\n",
        "  layer_dims=[16,32,64,128]\n",
        "  best_f1 = 0\n",
        "  best_recall = 0\n",
        "  best_precision = 0\n",
        "  best_auroc = 0\n",
        "\n",
        "  for drop_rate in drop_rates:\n",
        "    for layer in layer_dims:\n",
        "      model_mean = GCN_mean(hidden_channels=layer, dropout_frac=drop_rate).to(device)\n",
        "      optimizer = torch.optim.Adam(model_mean.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "      for _ in range(1, 501):\n",
        "          train(model_mean, train_dataloader, device, optimizer)\n",
        "\n",
        "      test_recall, test_precision, test_auroc, test_f1 = test(model_mean, test_dataloader, device)\n",
        "      if test_f1>best_f1:\n",
        "        best_f1=test_f1\n",
        "        best_recall=test_recall\n",
        "        best_precision=test_precision\n",
        "        best_auroc=test_auroc\n",
        "\n",
        "  current_metrics_df=pd.DataFrame()\n",
        "  current_metrics_df['Step']=[step]\n",
        "  current_metrics_df['Recall']=[best_recall]\n",
        "  current_metrics_df['Precision']=[best_precision]\n",
        "  current_metrics_df['Auroc']=[best_auroc]\n",
        "  current_metrics_df['F1']=[best_f1]\n",
        "  if metrics_df.shape[0]==0: \n",
        "    current_metrics_df.to_csv('Final_metrics_10_runs/metrics_GCN_mean.csv')\n",
        "    return current_metrics_df\n",
        "  else:\n",
        "    metrics_df = pd.concat([metrics_df, current_metrics_df])\n",
        "    metrics_df.to_csv('Final_metrics_10_runs/metrics_GCN_mean.csv')\n",
        "    return metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [26:05:14<00:00, 9391.47s/it]  \n"
          ]
        }
      ],
      "source": [
        "for step in tqdm(range(1,11)):\n",
        "    graphs_data, labels_data,graph_idx  = create_data(step)\n",
        "    train_dataloader, test_dataloader = data_splitter(graphs_data, labels_data, graph_idx)\n",
        "    if step == 1:\n",
        "        metrics_df_mean = do_step_mean(train_dataloader, test_dataloader, step)\n",
        "    else:\n",
        "        metrics_df_mean = do_step_mean(train_dataloader, test_dataloader, step,metrics_df_mean)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Teaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "td = complete_dataset.groupby('contract_address')[['from']].count().reset_index()\n",
        "td1 = td[(td['from']>=10) & (td['from']<20)]['contract_address']\n",
        "td2 = td[(td['from']>=20) & (td['from']<30)]['contract_address']\n",
        "td3 = td[(td['from']>=30) & (td['from']<40)]['contract_address']\n",
        "td4 = td[(td['from']>=40) & (td['from']<50)]['contract_address']\n",
        "td5 = td[(td['from']>=50) & (td['from']<60)]['contract_address']\n",
        "td6 = td[(td['from']>=60) & (td['from']<70)]['contract_address']\n",
        "td7 = td[(td['from']>=70) & (td['from']<80)]['contract_address']\n",
        "td8 = td[(td['from']>=80) & (td['from']<90)]['contract_address']\n",
        "td9 = td[(td['from']>=90) & (td['from']<100)]['contract_address']\n",
        "td10 = td[td['from']>=100]['contract_address']\n",
        "\n",
        "addresses_step1 = complete_dataset[complete_dataset['contract_address'].isin(td1)][['contract_address', 'is_ponzi']].drop_duplicates('contract_address').reset_index(drop=True).sample(frac=1).reset_index()\n",
        "train_df1, test_df1 = train_test_split(addresses_step1, test_size=0.25,random_state=42, stratify=addresses_step1['is_ponzi'])\n",
        "addresses_step2 = complete_dataset[complete_dataset['contract_address'].isin(td2)][['contract_address', 'is_ponzi']].drop_duplicates('contract_address').reset_index(drop=True).sample(frac=1).reset_index()\n",
        "train_df2, test_df2 = train_test_split(addresses_step2, test_size=0.25,random_state=42, stratify=addresses_step2['is_ponzi'])\n",
        "addresses_step3 = complete_dataset[complete_dataset['contract_address'].isin(td3)][['contract_address', 'is_ponzi']].drop_duplicates('contract_address').reset_index(drop=True).sample(frac=1).reset_index()\n",
        "train_df3, test_df3 = train_test_split(addresses_step3, test_size=0.25,random_state=42, stratify=addresses_step3['is_ponzi'])\n",
        "addresses_step4 = complete_dataset[complete_dataset['contract_address'].isin(td4)][['contract_address', 'is_ponzi']].drop_duplicates('contract_address').reset_index(drop=True).sample(frac=1).reset_index()\n",
        "train_df4, test_df4 = train_test_split(addresses_step4, test_size=0.25,random_state=42, stratify=addresses_step4['is_ponzi'])\n",
        "addresses_step5 = complete_dataset[complete_dataset['contract_address'].isin(td5)][['contract_address', 'is_ponzi']].drop_duplicates('contract_address').reset_index(drop=True).sample(frac=1).reset_index()\n",
        "train_df5, test_df5 = train_test_split(addresses_step5, test_size=0.25,random_state=42, stratify=addresses_step5['is_ponzi'])\n",
        "addresses_step6 = complete_dataset[complete_dataset['contract_address'].isin(td6)][['contract_address', 'is_ponzi']].drop_duplicates('contract_address').reset_index(drop=True).sample(frac=1).reset_index()\n",
        "train_df6, test_df6 = train_test_split(addresses_step6, test_size=0.25,random_state=42, stratify=addresses_step6['is_ponzi'])\n",
        "addresses_step7 = complete_dataset[complete_dataset['contract_address'].isin(td7)][['contract_address', 'is_ponzi']].drop_duplicates('contract_address').reset_index(drop=True).sample(frac=1).reset_index()\n",
        "train_df7, test_df7 = train_test_split(addresses_step7, test_size=0.25,random_state=42, stratify=addresses_step7['is_ponzi'])\n",
        "addresses_step8 = complete_dataset[complete_dataset['contract_address'].isin(td8)][['contract_address', 'is_ponzi']].drop_duplicates('contract_address').reset_index(drop=True).sample(frac=1).reset_index()\n",
        "train_df8, test_df8 = train_test_split(addresses_step8, test_size=0.25,random_state=42, stratify=addresses_step8['is_ponzi'])\n",
        "addresses_step9 = complete_dataset[complete_dataset['contract_address'].isin(td9)][['contract_address', 'is_ponzi']].drop_duplicates('contract_address').reset_index(drop=True).sample(frac=1).reset_index()\n",
        "train_df9, test_df9 = train_test_split(addresses_step9, test_size=0.25,random_state=42, stratify=addresses_step9['is_ponzi'])\n",
        "addresses_step10 = complete_dataset[complete_dataset['contract_address'].isin(td10)][['contract_address', 'is_ponzi']].drop_duplicates('contract_address').reset_index(drop=True).sample(frac=1).reset_index()\n",
        "train_df10, test_df10 = train_test_split(addresses_step10, test_size=0.25,random_state=42, stratify=addresses_step10['is_ponzi'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_df_calculator(step):\n",
        "    if step == 1:\n",
        "        train_dfs = pd.concat([train_df1, train_df2, train_df3, train_df4, train_df5, train_df6, train_df7, train_df8, train_df9, train_df10], axis=0)\n",
        "        test_dfs = pd.concat([test_df1, test_df2, test_df3, test_df4, test_df5, test_df6, test_df7, test_df8, test_df9, test_df10], axis=0)\n",
        "        return train_dfs, test_dfs\n",
        "    elif step == 2:\n",
        "        train_dfs = pd.concat([train_df2, train_df3, train_df4, train_df5, train_df6, train_df7, train_df8, train_df9, train_df10], axis=0)\n",
        "        test_dfs = pd.concat([test_df2, test_df3, test_df4, test_df5, test_df6, test_df7, test_df8, test_df9, test_df10], axis=0)\n",
        "        return train_dfs, test_dfs\n",
        "    elif step == 3:\n",
        "        train_dfs = pd.concat([train_df3, train_df4, train_df5, train_df6, train_df7, train_df8, train_df9, train_df10], axis=0)\n",
        "        test_dfs = pd.concat([test_df3, test_df4, test_df5, test_df6, test_df7, test_df8, test_df9, test_df10], axis=0)\n",
        "        return train_dfs, test_dfs\n",
        "    elif step == 4:\n",
        "        train_dfs = pd.concat([train_df4, train_df5, train_df6, train_df7, train_df8, train_df9, train_df10], axis=0)\n",
        "        test_dfs = pd.concat([test_df4, test_df5, test_df6, test_df7, test_df8, test_df9, test_df10], axis=0)\n",
        "        return train_dfs, test_dfs\n",
        "    elif step == 5:\n",
        "        train_dfs = pd.concat([train_df5, train_df6, train_df7, train_df8, train_df9, train_df10], axis=0)\n",
        "        test_dfs = pd.concat([test_df5, test_df6, test_df7, test_df8, test_df9, test_df10], axis=0)\n",
        "        return train_dfs, test_dfs\n",
        "    elif step == 6:\n",
        "        train_dfs = pd.concat([train_df6, train_df7, train_df8, train_df9, train_df10], axis=0)\n",
        "        test_dfs = pd.concat([test_df6, test_df7, test_df8, test_df9, test_df10], axis=0)\n",
        "        return train_dfs, test_dfs\n",
        "    elif step == 7:\n",
        "        train_dfs = pd.concat([train_df7, train_df8, train_df9, train_df10], axis=0)\n",
        "        test_dfs = pd.concat([test_df7, test_df8, test_df9, test_df10], axis=0)\n",
        "        return train_dfs, test_dfs\n",
        "    elif step == 8:\n",
        "        train_dfs = pd.concat([train_df8, train_df9, train_df10], axis=0)\n",
        "        test_dfs = pd.concat([test_df8, test_df9, test_df10], axis=0)\n",
        "        return train_dfs, test_dfs\n",
        "    elif step == 9:\n",
        "        train_dfs = pd.concat([train_df9, train_df10], axis=0)\n",
        "        test_dfs = pd.concat([test_df9, test_df10], axis=0)\n",
        "        return train_dfs, test_dfs\n",
        "    else:\n",
        "        return train_df10, test_df10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "complete_dataset=complete_dataset.fillna(0)\n",
        "\n",
        "def create_data_teaug(step,data_togen):\n",
        "  graphs_data = []\n",
        "  labels_data = []\n",
        "  graph_idx = []\n",
        "  for contract in list(set(data_togen['contract_address'])):\n",
        "      temp_df = complete_dataset[complete_dataset['contract_address']==contract].reset_index(drop=True).head(10*step)\n",
        "      if temp_df.shape[0]>=10*step:\n",
        "        G=nx.DiGraph()\n",
        "        #nx.set_node_attributes(G, {node:{x:[attributes]}})\n",
        "        #adding node features\n",
        "        node_attributes = pd.merge(transaction_values_sent(temp_df), transaction_values_received(temp_df), on='identifier', how='outer')\n",
        "        node_attributes['account_balance']=node_attributes.loc[:, ('Value_usd_x', 'sum')] - node_attributes.loc[:, ('Value_usd_y', 'sum')]\n",
        "        node_attributes = pd.merge(node_attributes, errors_sent(temp_df), on='identifier', how='outer')\n",
        "        node_attributes = pd.merge(node_attributes, errors_received(temp_df), on='identifier', how='outer')\n",
        "        node_attributes = pd.merge(node_attributes, lifetimes_sent(temp_df), on='identifier', how='outer')\n",
        "        node_attributes = pd.merge(node_attributes, lifetimes_received(temp_df), on='identifier', how='outer')\n",
        "        node_attributes = pd.merge(node_attributes, transaction_costs_sent(temp_df), on='identifier', how='outer')\n",
        "        #node_attributes['distinct_receiving_users'] = temp_df.groupby('from')[['to']].nunique()['to'].tolist()\n",
        "        cluster_data = temp_df.iloc[0, -6:].tolist()\n",
        "        node_attributes['cluster_0']=cluster_data[0]\n",
        "        node_attributes['cluster_1']=cluster_data[1]\n",
        "        node_attributes['cluster_2']=cluster_data[2]\n",
        "        node_attributes['cluster_3']=cluster_data[3]\n",
        "        node_attributes['cluster_4']=cluster_data[4]\n",
        "        node_attributes['cluster_5']=cluster_data[5]\n",
        "        node_attributes=node_attributes.fillna(0)\n",
        "\n",
        "        aggregated_edges = aggregate_edges(temp_df)\n",
        "\n",
        "        for j in range(aggregated_edges.shape[0]):\n",
        "          #adding edges with their feaures\n",
        "          from_node = aggregated_edges.loc[j, 'from']\n",
        "          to_node = aggregated_edges.loc[j, 'to']\n",
        "          edge_characts = aggregated_edges.loc[j, 'value']\n",
        "          G.add_edge(from_node, to_node, edge_attr=edge_characts)\n",
        "\n",
        "\n",
        "        for idx in node_attributes['identifier']:\n",
        "          attrs = {idx: {'x':node_attributes[node_attributes['identifier']==idx].iloc[0, :].tolist()[1:]}}\n",
        "          nx.set_node_attributes(G, attrs)\n",
        "\n",
        "        temp_df['value_correct_sign'] = temp_df.apply(sign_calculator, axis=1)\n",
        "        temp_df_internal = temp_df[temp_df['isinternal']==1]\n",
        "        temp_df_external = temp_df[temp_df['isinternal']==0]\n",
        "        graph_attributes = []\n",
        "        #degrees\n",
        "        contract_in_degrees=degree_calculator(temp_df_external)\n",
        "        contract_out_degrees=degree_calculator(temp_df_internal)\n",
        "        #number_transacts_users_indeg\n",
        "        avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = number_transacts_user(temp_df_external)\n",
        "        graph_attributes+=[avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #number_transacts_users_outdeg\n",
        "        avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = number_transacts_user(temp_df_internal)\n",
        "        graph_attributes+=[avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #errors_indeg\n",
        "        total_numb,avg_numb, std_numb, median_numb, q1_numb, q3_numb = errors_calculator(temp_df_external)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, q1_numb, q3_numb]\n",
        "        #errors_outdeg\n",
        "        total_numb,avg_numb, std_numb, median_numb, q1_numb, q3_numb = errors_calculator(temp_df_internal)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, q1_numb, q3_numb]\n",
        "        #distinct_users\n",
        "        distinct_users_indeg=distinct_users(temp_df_external)\n",
        "        distinct_users_outdeg=distinct_users(temp_df_internal)\n",
        "        graph_attributes+=[distinct_users_indeg, distinct_users_outdeg]\n",
        "        #transaction_value_indeg\n",
        "        total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = transaction_value(temp_df_external)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #transaction_value_outdeg\n",
        "        total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = transaction_value(temp_df_internal)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #age_minutes\n",
        "        avg_numb, std_numb, median_numb, q1_numb, q3_numb, max_numb = lifetime_calculator(temp_df)\n",
        "        graph_attributes+=[avg_numb, std_numb, median_numb, q1_numb, q3_numb, max_numb]\n",
        "        #inflows_from_users\n",
        "        avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = inflows_from_users(temp_df_external)\n",
        "        graph_attributes+=[avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #outflows_from_users\n",
        "        avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = outflows_to_users(temp_df_internal)\n",
        "        graph_attributes+=[avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #transaction_costs_contract\n",
        "        total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = transaction_costs_contract(temp_df_internal)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        #transaction_costs_users\n",
        "        avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb = transaction_costs_users(temp_df_external)\n",
        "        graph_attributes+=[total_numb,avg_numb, std_numb, median_numb, min_numb, q1_numb, q3_numb, max_numb]\n",
        "        G.graph.update({'contract_features':graph_attributes})\n",
        "\n",
        "        label = temp_df.is_ponzi.iloc[0]\n",
        "        graphs_data.append(G)\n",
        "        labels_data.append(label)\n",
        "        graph_idx.append(contract)\n",
        "  return graphs_data, labels_data, graph_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_splitter_teaug(step, prev_train_data_graph=list(), prev_train_data_label=None, prev_train_data_idx=None):\n",
        "  train, test = train_test_df_calculator(step)\n",
        "  train_data_graph, train_data_label, train_data_idx = create_data_teaug(step, train)\n",
        "  test_data_graph, test_data_label, test_data_idx = create_data_teaug(step, test)\n",
        "  if len(prev_train_data_graph)>0:\n",
        "    train_data_graph+=prev_train_data_graph\n",
        "    train_data_label+=prev_train_data_label\n",
        "    train_data_idx+=prev_train_data_idx\n",
        "  dataset_train = FraudDataset(train_data_graph, train_data_label, train_data_idx)\n",
        "  dataset_test = FraudDataset(test_data_graph, test_data_label, test_data_idx)\n",
        "  train_dataloader = DataLoader(dataset_train, batch_size=500, shuffle=True)\n",
        "  test_dataloader = DataLoader(dataset_test, batch_size=100, shuffle=False)\n",
        "  return train_dataloader, test_dataloader, train_data_graph, train_data_label, train_data_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def do_step_mean_teaug(train_dataloader, test_dataloader, step, metrics_df = pd.DataFrame()):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  drop_rates = [0.1,0.2,0.3,0.4]\n",
        "  layer_dims=[16,32,64,128]\n",
        "  best_f1 = 0\n",
        "  best_recall = 0\n",
        "  best_precision = 0\n",
        "  best_auroc = 0\n",
        "\n",
        "  for drop_rate in drop_rates:\n",
        "    for layer in layer_dims:\n",
        "      model_mean = GCN_mean(hidden_channels=layer, dropout_frac=drop_rate).to(device)\n",
        "      optimizer = torch.optim.Adam(model_mean.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "      for _ in range(1, 501):\n",
        "          train(model_mean, train_dataloader, device, optimizer)\n",
        "\n",
        "      test_recall, test_precision, test_auroc, test_f1 = test(model_mean, test_dataloader, device)\n",
        "      if test_f1>best_f1:\n",
        "        best_f1=test_f1\n",
        "        best_recall=test_recall\n",
        "        best_precision=test_precision\n",
        "        best_auroc=test_auroc\n",
        "\n",
        "  current_metrics_df=pd.DataFrame()\n",
        "  current_metrics_df['Step']=[step]\n",
        "  current_metrics_df['Recall']=[best_recall]\n",
        "  current_metrics_df['Precision']=[best_precision]\n",
        "  current_metrics_df['Auroc']=[best_auroc]\n",
        "  current_metrics_df['F1']=[best_f1]\n",
        "  if metrics_df.shape[0]==0: \n",
        "    current_metrics_df.to_csv('Final_metrics_10_runs/metrics_GCN_mean_TEAUG.csv')\n",
        "    return current_metrics_df\n",
        "  else:\n",
        "    metrics_df = pd.concat([metrics_df, current_metrics_df])\n",
        "    metrics_df.to_csv('Final_metrics_10_runs/metrics_GCN_mean_TEAUG.csv')\n",
        "    return metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [129:32:41<00:00, 46636.16s/it]  \n"
          ]
        }
      ],
      "source": [
        "for step in tqdm(range(1,11)):\n",
        "    if step == 1:\n",
        "        train_dataloader, test_dataloader, train_data_graph, train_data_label, train_data_idx = data_splitter_teaug(step)\n",
        "        metrics_df_mean = do_step_mean_teaug(train_dataloader, test_dataloader, step)\n",
        "    else:\n",
        "        train_dataloader, test_dataloader, train_data_graph, train_data_label, train_data_idx = data_splitter_teaug(step, train_data_graph, train_data_label, train_data_idx)\n",
        "        metrics_df_mean = do_step_mean_teaug(train_dataloader, test_dataloader, step,metrics_df_mean)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "reu_RhfEhMYy"
      },
      "source": [
        "## GATConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "class GAT_mean(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, dropout_frac=0.5):\n",
        "        super(GAT_mean, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "        self.conv1 = GATConv(41, hidden_channels, heads=1) #41=node features\n",
        "        self.lin1 = Linear(hidden_channels+80, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, 2) #2=number of classes\n",
        "        self.dropout_frac=dropout_frac\n",
        "\n",
        "    def forward(self, data):\n",
        "        x=data.x.to(torch.float32)\n",
        "        x = self.conv1(x, data.edge_index)\n",
        "        x = x.relu()\n",
        "        x = global_mean_pool(x, data.batch)\n",
        "        contract_feats=data.contract_features.to(torch.float32)\n",
        "        contract_feats = contract_feats.reshape(len(data.label), 80)\n",
        "        contract_feats[torch.isnan(contract_feats)] = 0\n",
        "        x = torch.cat((x, contract_feats), dim=1)\n",
        "        x =self.lin1(x)\n",
        "        x = F.dropout(x, p=self.dropout_frac, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "def train(model, train_dataloader, device, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    for data in train_dataloader:  # Iterate in batches over the training dataset.\n",
        "         data = data.to(device)\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "         out = model(data)  # Perform a single forward pass.\n",
        "         y_label = torch.tensor(data.label).to(device)\n",
        "         #loss = F.nll_loss(out, y_label)\n",
        "         loss = F.nll_loss(out, y_label)\n",
        "         #loss = criterion(out, y_label)  # Compute the loss.\n",
        "         #loss=criterion(torch.argmax(out, dim=1), y_label)\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "\n",
        "def test(model, loader, device):\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  outs = []\n",
        "  true_y=[]\n",
        "  for data in loader:\n",
        "      data = data.to(device)\n",
        "      out = model(data)\n",
        "      pred = out.argmax(dim=1)\n",
        "      preds+=pred.tolist()\n",
        "      true_y+=data.label\n",
        "      outs+=out.tolist()\n",
        "  outs=np.array(outs)\n",
        "  return recall_score(true_y, preds), precision_score(true_y, preds), roc_auc_score(true_y,outs[:,1]), f1_score(true_y, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "G3fJXdJnhMvR"
      },
      "outputs": [],
      "source": [
        "def do_step_gat_mean(train_dataloader, test_dataloader, step, metrics_df = pd.DataFrame()):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  drop_rates = [0.1,0.2,0.3,0.4]\n",
        "  layer_dims=[16,32,64,128]\n",
        "  best_f1 = 0\n",
        "  best_recall = 0\n",
        "  best_precision = 0\n",
        "  best_auroc = 0\n",
        "\n",
        "  for drop_rate in drop_rates:\n",
        "    for layer in layer_dims:\n",
        "      model_mean = GAT_mean(hidden_channels=layer, dropout_frac=drop_rate).to(device)\n",
        "      optimizer = torch.optim.Adam(model_mean.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "      for _ in range(1, 501):\n",
        "          train(model_mean, train_dataloader, device, optimizer)\n",
        "\n",
        "      test_recall, test_precision, test_auroc, test_f1 = test(model_mean, test_dataloader, device)\n",
        "      if test_f1>best_f1:\n",
        "        best_f1=test_f1\n",
        "        best_recall=test_recall\n",
        "        best_precision=test_precision\n",
        "        best_auroc=test_auroc\n",
        "\n",
        "  current_metrics_df=pd.DataFrame()\n",
        "  current_metrics_df['Step']=[step]\n",
        "  current_metrics_df['Recall']=[best_recall]\n",
        "  current_metrics_df['Precision']=[best_precision]\n",
        "  current_metrics_df['Auroc']=[best_auroc]\n",
        "  current_metrics_df['F1']=[best_f1]\n",
        "  if metrics_df.shape[0]==0: \n",
        "    current_metrics_df.to_csv('Final_metrics_10_runs/metrics_GAT_mean.csv')\n",
        "    return current_metrics_df\n",
        "  else:\n",
        "    metrics_df = pd.concat([metrics_df, current_metrics_df])\n",
        "    metrics_df.to_csv('Final_metrics_10_runs/metrics_GAT_mean.csv')\n",
        "    return metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [28:52:48<00:00, 10396.83s/it] \n"
          ]
        }
      ],
      "source": [
        "for step in tqdm(range(1,11)):\n",
        "    graphs_data, labels_data,graph_idx  = create_data(step)\n",
        "    train_dataloader, test_dataloader = data_splitter(graphs_data, labels_data, graph_idx)\n",
        "    if step == 1:\n",
        "        metrics_df_mean = do_step_gat_mean(train_dataloader, test_dataloader, step)\n",
        "    else:\n",
        "        metrics_df_mean = do_step_gat_mean(train_dataloader, test_dataloader, step,metrics_df_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def do_step_gat_mean_teaug(train_dataloader, test_dataloader, step, metrics_df = pd.DataFrame()):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  drop_rates = [0.1,0.2,0.3,0.4]\n",
        "  layer_dims=[16,32,64,128]\n",
        "  best_f1 = 0\n",
        "  best_recall = 0\n",
        "  best_precision = 0\n",
        "  best_auroc = 0\n",
        "\n",
        "  for drop_rate in drop_rates:\n",
        "    for layer in layer_dims:\n",
        "      model_mean = GAT_mean(hidden_channels=layer, dropout_frac=drop_rate).to(device)\n",
        "      optimizer = torch.optim.Adam(model_mean.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "      for _ in range(1, 501):\n",
        "          train(model_mean, train_dataloader, device, optimizer)\n",
        "\n",
        "      test_recall, test_precision, test_auroc, test_f1 = test(model_mean, test_dataloader, device)\n",
        "      if test_f1>best_f1:\n",
        "        best_f1=test_f1\n",
        "        best_recall=test_recall\n",
        "        best_precision=test_precision\n",
        "        best_auroc=test_auroc\n",
        "\n",
        "  current_metrics_df=pd.DataFrame()\n",
        "  current_metrics_df['Step']=[step]\n",
        "  current_metrics_df['Recall']=[best_recall]\n",
        "  current_metrics_df['Precision']=[best_precision]\n",
        "  current_metrics_df['Auroc']=[best_auroc]\n",
        "  current_metrics_df['F1']=[best_f1]\n",
        "  if metrics_df.shape[0]==0: \n",
        "    current_metrics_df.to_csv('Final_metrics_10_runs/metrics_GAT_mean_TEAUG.csv')\n",
        "    return current_metrics_df\n",
        "  else:\n",
        "    metrics_df = pd.concat([metrics_df, current_metrics_df])\n",
        "    metrics_df.to_csv('Final_metrics_10_runs/metrics_GAT_mean_TEAUG.csv')\n",
        "    return metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [128:33:40<00:00, 46282.06s/it]  \n"
          ]
        }
      ],
      "source": [
        "for step in tqdm(range(1,11)):\n",
        "    if step == 1:\n",
        "        train_dataloader, test_dataloader, train_data_graph, train_data_label, train_data_idx = data_splitter_teaug(step)\n",
        "        metrics_df_mean = do_step_gat_mean_teaug(train_dataloader, test_dataloader, step)\n",
        "    else:\n",
        "        train_dataloader, test_dataloader, train_data_graph, train_data_label, train_data_idx = data_splitter_teaug(step, train_data_graph, train_data_label, train_data_idx)\n",
        "        metrics_df_mean = do_step_gat_mean_teaug(train_dataloader, test_dataloader, step,metrics_df_mean)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "reu_RhfEhMYy"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "6f59339097bb9ac4cf41ab9fa2e7f783ea6bb84442f6ce2c2671fa41ded377c2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
